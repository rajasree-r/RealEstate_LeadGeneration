{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Import library and models \n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import xlrd;\n",
    "import openpyxl;\n",
    "\n",
    "# Execute command 'pip install info_gain' and 'pip install matplotlib'\n",
    "!pip install info-gain\n",
    "!pip install matplotlib\n",
    "\n",
    "####### Part 1. Data Preprocessing\n",
    "\n",
    "##### 1.1 Change the original 4 '.csv' data files into '.xlsx' format.\n",
    "\n",
    "##### 1.2 Import Data\n",
    "\n",
    "### Read the 4 original data files into pandas data frame.\n",
    "deals = pd.read_excel(\"deals.xlsx\")\n",
    "clients = pd.read_excel(\"clients.xlsx\")\n",
    "assets = pd.read_excel(\"assets.xlsx\")\n",
    "\n",
    "# Change the field name 'assetid' to 'AssetID'. Otherwise, it will have problem in grouping statistics.\n",
    "asset_transactions = pd.read_excel(\"asset_transaction_history.xlsx\")\n",
    "\n",
    "### Print the record count and head names of each file.\n",
    "print (\"The record count of 'deals' table is: \")\n",
    "print (deals.shape[0])\n",
    "\n",
    "print (\"The record count of 'clients' table is: \")\n",
    "print (clients.shape[0])\n",
    "\n",
    "print (\"The record count of 'assets' table is: \")\n",
    "print (assets.shape[0])\n",
    "\n",
    "print (\"The record count of 'asset_transactions' table is: \")\n",
    "print (asset_transactions.shape[0])\n",
    "\n",
    "\n",
    "##### 1.3 Priliminary filters asset transaction frame to get IA asset transactions.\n",
    "\n",
    "### Remove the deal without any effective date (AccountingDate, BuyerSelectionDate, InitialBidDueDate, MarketingDate, HireDate).\n",
    "\n",
    "# Get valid deals.\n",
    "valid_deals = deals [(deals.AccountingDate.notnull()) | (deals.BuyerSelectionDate.notnull()) | (deals.InitialBidDueDate.notnull()) | (deals.MarketingDate.notnull()) | (deals.HireDate.notnull())].drop_duplicates()\n",
    "print (\"The valid record count of 'deals' table is: \")\n",
    "print (valid_deals.shape[0])\n",
    "\n",
    "# Get valid IA asset transactions.\n",
    "ia_asset_transactions = asset_transactions [asset_transactions.DealID.isin (valid_deals.DealID)].drop_duplicates()\n",
    "print (\"The valid record count of 'ia_asset_transactions' table is: \")\n",
    "print (ia_asset_transactions.shape[0])\n",
    "\n",
    "# Get valid customer count for each IA asset transaction.\n",
    "grouped_ia_transactions = ia_asset_transactions.groupby(['DealID', 'AssetID']).size().reset_index(name='ClientCount').drop_duplicates()\n",
    "print (\"The valid record count of 'grouped_asset_transactions' table is: \")\n",
    "print (grouped_ia_transactions.shape[0])\n",
    "\n",
    "# Get client data only for IA transactions.\n",
    "ia_clients = clients [clients.ClientID.isin (ia_asset_transactions.ClientID)].drop_duplicates()\n",
    "print (\"The record count of 'ia_clients' table is: \")\n",
    "print (ia_clients.shape[0])\n",
    "\n",
    "# Get the unique asset data for IA transactions.\n",
    "ia_assets = assets [assets.AssetID.isin (grouped_ia_transactions.AssetID)].drop_duplicates()\n",
    "print (\"The record count of 'ia_assets' table is: \")\n",
    "print (ia_assets.shape[0])\n",
    "\n",
    "# Get the unique asset data for non-IA transactions.\n",
    "not_ia_assets = assets [~assets.AssetID.isin (grouped_ia_transactions.AssetID)].drop_duplicates()\n",
    "print (\"The record count of 'not_ia_assets' table is: \")\n",
    "print (not_ia_assets.shape[0])\n",
    "\n",
    "# Save the priliminarily processed data to the below .xlsx files.\n",
    "valid_deals.to_excel('ia_deals.xlsx')\n",
    "ia_asset_transactions.to_excel('ia_asset_transactions.xlsx')\n",
    "grouped_ia_transactions.to_excel('ia_asset_transactions_group.xlsx')\n",
    "ia_clients.to_excel('ia_clients.xlsx')\n",
    "ia_assets.to_excel('ia_assets.xlsx')\n",
    "not_ia_assets.to_excel('ia_not_assets.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data quality issue exists. For example, For assetID : 202484, the build date is 2018, but the market data is 2017 year, so the 'time to market' is negative values.\n",
    "## Delete all similar 'Date to market'record values (<= 180)in dataset\n",
    "## Keep those abnormal value for analysis\n",
    "\n",
    "### Data quality problem: AssetID 165000\n",
    "\n",
    "import datetime as dt;\n",
    "\n",
    "##### 1.4 Genenrate 'Date_to_Market' and 'Date_to_Predict'data for all assets with market transaction records.\n",
    "\n",
    "### Assume the 'Built' and 'Renovation' Month_Data to be (01-01) instead of (06-30) to reduce data quality issue.\n",
    "\n",
    "# Calculate the property 'Built' Year, Date and Time.\n",
    "def date_build(row):\n",
    "    return dt.datetime(int(row['Year_Built']), 1, 1)\n",
    "\n",
    "# Calculate the property 'Renovation' Year, Date and Time.\n",
    "def date_renovation (row):\n",
    "    if row['Year_Renovated'] > 0:\n",
    "       return dt.datetime(row['Year_Renovated'], 1, 1)\n",
    "    else:\n",
    "       return np.NAN\n",
    "\n",
    "# Calculate the interval no. of date from property 'Built' till now.\n",
    "def date_from_built(row):\n",
    "    return (row['Current_Date'] - row['Date_Built']).days\n",
    "\n",
    "# Calculate the interval no. of date from property 'Renovation' till now.\n",
    "def date_from_renovation(row):\n",
    "    if row['Year_Renovated'] == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return (row['Prediction_Date'] - row['Date_Renovated']).days\n",
    "    \n",
    "\n",
    "### Merge deals and assets to form a large 'Asset - Deal - Date_to_Market' data frame.\n",
    "\n",
    "# Merge 'valid_deals' with 'grouped_ia_asset_transactions'.\n",
    "merge_deals_assets_0 = pd.merge(valid_deals, grouped_ia_transactions, on='DealID', how = 'inner').drop_duplicates()\n",
    "\n",
    "# Continue to merge the above result data frame with 'ia_assets'.\n",
    "merge_deals_assets_1 = pd.merge(merge_deals_assets_0, ia_assets, on='AssetID',  how = 'left').drop_duplicates()\n",
    "\n",
    "merge_deals_assets_1.rename(columns={'PropertyType_y': 'PropertyType'}, inplace=True)\n",
    "\n",
    "# Delete the columns that result in the data quality problem: 'MSACode', 'Class', and 'Amount'          \n",
    "deal_asset_time = merge_deals_assets_1 [['AssetID', 'ClientCount', 'City', 'State', 'CountryID', 'CityID', 'State.1', 'Country',\n",
    "                                         'Zip', 'PropertyTypeID', 'PropertyType', 'PropertyTypeGroup', 'YearBuilt', 'AssetLatitude',\n",
    "                                         'AssetLongitude', 'LandAreaInAcres', 'NumberofFloors', 'NumberofBuildings', 'YearRenovated',\n",
    "                                         'Size_Acres', 'Size_InLineSqft', 'IsPortfolio', 'PortfolioCount', 'Lat_city', 'Long_city',\n",
    "                                         'IsGateway', 'Region', 'Size_Sqft', 'Size_Units', 'PropertySize', 'UnitType', 'ClassDateStamp',\n",
    "                                         'Market_name', 'CityID.1', 'DealID', 'AccountingDate', 'BuyerSelectionDate', 'InitialBidDueDate',\n",
    "                                         'MarketingDate', 'HireDate']].drop_duplicates()\n",
    "\n",
    "# Column 'YearBuilt' is very important for modeling and analaysis.\n",
    "# Drop the rows with 'YearBuilt' value (null or <1700 or >2019) in the merged data frame\n",
    "# Drop the rows with 'YearRenovated' value (<1700 or > 2019)\n",
    "deal_asset_time = deal_asset_time [(deal_asset_time.YearBuilt.notnull()) & (deal_asset_time.YearBuilt < 2019) & (deal_asset_time.YearBuilt >= 1700) & (deal_asset_time.YearRenovated.isnull() | ((deal_asset_time.YearRenovated < 2019) & (deal_asset_time.YearRenovated >= 1700)))]\n",
    "\n",
    "# Calculate 'Deal_Start_Date' to the the least of five data value (AccountingDate, BuyerSelectionDate, InitialBidDueDate, MarketingDate, HireDate).\n",
    "deal_asset_time['Deal_Start_Date'] = pd.to_datetime(deal_asset_time[['AccountingDate', 'BuyerSelectionDate', 'InitialBidDueDate',\n",
    "                                         'MarketingDate', 'HireDate']].min(axis=1))\n",
    "\n",
    "# Calculate 'Deal_Start_Date' to the the largest of five data value (AccountingDate, BuyerSelectionDate, InitialBidDueDate, MarketingDate, HireDate)\n",
    "deal_asset_time['Deal_End_Date'] = pd.to_datetime(deal_asset_time[['AccountingDate', 'BuyerSelectionDate', 'InitialBidDueDate',\n",
    "                                         'MarketingDate', 'HireDate']].max(axis=1))\n",
    "\n",
    "# Calculate the 'Current_Date' and 'Prediction_Date' (1 year after the current date). \n",
    "deal_asset_time['Current_Date'] = pd.Timestamp(\"today\")\n",
    "deal_asset_time['Prediction_Date'] = pd.Timestamp(\"today\") + pd.DateOffset(years=1)\n",
    "\n",
    "# Calculate the value for columns 'Year_Built', 'Year_Renovated', 'Date_Built', 'Date_Renovated','Date_from_Built' and 'Date_from_Renovation'.\n",
    "deal_asset_time['Year_Built'] = pd.to_numeric(deal_asset_time['YearBuilt'], downcast='signed').astype(np.int)\n",
    "deal_asset_time['Year_Renovated'] = pd.to_numeric(np.where(deal_asset_time ['YearRenovated'].isnull(), -1, deal_asset_time['YearRenovated']), downcast='signed').astype(np.int)\n",
    "deal_asset_time['Date_Built'] = deal_asset_time.apply (lambda row: date_build(row),axis=1)\n",
    "deal_asset_time['Date_Renovated'] = deal_asset_time.apply (lambda row: date_renovation(row),axis=1)\n",
    "deal_asset_time['Date_from_Built'] = deal_asset_time.apply (lambda row: date_from_built(row),axis=1)\n",
    "deal_asset_time['Date_from_Renovation'] = deal_asset_time.apply (lambda row: date_from_renovation(row),axis=1)\n",
    "\n",
    "# Order the result data frame by 'AssetID','Deal_Start_Date' and 'Deal_End_Date' ascending orders.\n",
    "deal_asset_time_order = deal_asset_time.sort_values(by = ['AssetID','Deal_Start_Date','Deal_End_Date'])\n",
    "\n",
    "# Calculate the row no. for 'deal_asset_time_order' data frame\n",
    "total_rows = deal_asset_time_order.shape[0]\n",
    "\n",
    "# Add two columns to calculate the interval start date, and interval deal end data between 2 adjacent rows.\n",
    "# Set up the initial value to be a very big value.\n",
    "deal_asset_time_order['Interval_Deal_Start_Date'] = 100000\n",
    "deal_asset_time_order['Interval_Deal_End_Date'] = 100000\n",
    "\n",
    "# Use the loop to calculate the interval of 'Deal_Start_Date' and 'Deal_End_Date' between 2 neighbor records \n",
    "# The value of column 'Interval_Deal_Start_Date' and 'Interval_Deal_End_Date' will be used to judge if data quality issue exists (Mainly refers to the duplicate transaction records.)\n",
    "for row in range(1, total_rows):\n",
    "    if deal_asset_time_order['AssetID'].iloc[row] == deal_asset_time_order['AssetID'].iloc[row-1]:\n",
    "        deal_asset_time_order['Interval_Deal_Start_Date'].iloc[row] = abs((pd.to_datetime(deal_asset_time_order['Deal_Start_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order['Deal_Start_Date'].iloc[row-1])).days)\n",
    "        deal_asset_time_order['Interval_Deal_End_Date'].iloc[row] = abs((pd.to_datetime(deal_asset_time_order['Deal_End_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order['Deal_End_Date'].iloc[row-1])).days)\n",
    "\n",
    "        if deal_asset_time_order['Interval_Deal_Start_Date'].iloc[row] <= 10:\n",
    "            deal_asset_time_order['Interval_Deal_Start_Date'].iloc[row] = np.NAN\n",
    "\n",
    "        if deal_asset_time_order['Interval_Deal_End_Date'].iloc[row] <= 10:\n",
    "            deal_asset_time_order['Interval_Deal_End_Date'].iloc[row] = np.NAN\n",
    "\n",
    "\n",
    "# Filter the rows with data quality issues (Duplicate records) by the values of 'Deal_Start_Date' and 'Deal_End_Date'\n",
    "deal_asset_time_order_filter = deal_asset_time_order[deal_asset_time_order.Interval_Deal_Start_Date.notnull() & deal_asset_time_order.Interval_Deal_End_Date.notnull()].sort_values(by = ['AssetID','Deal_Start_Date','Deal_End_Date']).drop_duplicates()\n",
    "\n",
    "# Add 2 columns: 'Date_to_Market' and 'Date_to_Predict'.\n",
    "deal_asset_time_order_filter['Date_to_Market'] = np.NAN\n",
    "deal_asset_time_order_filter['Date_to_Predict'] = np.NAN\n",
    "\n",
    "# Calculate the row no. for 'deal_asset_time_order_filter' data frame\n",
    "new_total_rows = deal_asset_time_order_filter.shape[0]\n",
    "\n",
    "# Calculate the value of 'Date_to_Market' and 'Date_to_Predict' in the first row.\n",
    "deal_asset_time_order_filter['Date_to_Market'].iloc[0] = (pd.to_datetime(deal_asset_time_order_filter['Deal_Start_Date'].iloc[0]) - pd.to_datetime(deal_asset_time_order_filter['Date_Built'].iloc[0])).days\n",
    "if deal_asset_time_order_filter['Interval_Deal_Start_Date'].iloc[1] == 100000:\n",
    "    deal_asset_time_order_filter['Date_to_Predict'].iloc[0] = (pd.to_datetime(deal_asset_time_order_filter['Prediction_Date'].iloc[0]) - pd.to_datetime(deal_asset_time_order_filter['Deal_End_Date'].iloc[0])).days\n",
    "\n",
    "# Use loop to calculate the value of 'Date_to_Market' and 'Date_to_Predict' in all other rows.\n",
    "for row in range(1, new_total_rows):\n",
    "    if deal_asset_time_order_filter['Interval_Deal_Start_Date'].iloc[row] == 100000 and deal_asset_time_order_filter['Interval_Deal_End_Date'].iloc[row] == 100000:\n",
    "\n",
    "        deal_asset_time_order_filter['Date_to_Market'].iloc[row] = (pd.to_datetime(deal_asset_time_order_filter['Deal_Start_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order_filter['Date_Built'].iloc[row])).days\n",
    "        if row != (new_total_rows - 1):\n",
    "            if deal_asset_time_order_filter['Interval_Deal_Start_Date'].iloc[row + 1] == 100000:\n",
    "                deal_asset_time_order_filter['Date_to_Predict'].iloc[row] = (pd.to_datetime(deal_asset_time_order_filter['Prediction_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order_filter['Deal_End_Date'].iloc[row])).days\n",
    "        else:\n",
    "            deal_asset_time_order_filter['Date_to_Predict'].iloc[row] = (pd.to_datetime(deal_asset_time_order_filter['Prediction_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order_filter['Deal_End_Date'].iloc[row])).days\n",
    "    else:\n",
    "        deal_asset_time_order_filter['Date_to_Market'].iloc[row] = (pd.to_datetime(deal_asset_time_order_filter['Deal_Start_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order_filter['Deal_End_Date'].iloc[row-1])).days\n",
    "\n",
    "        if row != (new_total_rows - 1):\n",
    "            if deal_asset_time_order_filter['Interval_Deal_Start_Date'].iloc[row + 1] == 100000:\n",
    "                deal_asset_time_order_filter['Date_to_Predict'].iloc[row] = (pd.to_datetime(deal_asset_time_order_filter['Prediction_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order_filter['Deal_End_Date'].iloc[row])).days\n",
    "        else:\n",
    "            deal_asset_time_order_filter['Date_to_Predict'].iloc[row] = (pd.to_datetime(deal_asset_time_order_filter['Prediction_Date'].iloc[row]) - pd.to_datetime(deal_asset_time_order_filter['Deal_End_Date'].iloc[row])).days\n",
    "\n",
    "# Add the column \"Is_Transaction_Exist\", and set the value of each row to be 1 (Assets with market transaction).\n",
    "deal_asset_time_order_filter[\"Is_Transaction_Exist\"] = 1\n",
    "\n",
    "# Save this field for further data visualization using 'R'\n",
    "deal_asset_time_order_filter.to_csv('ia_deal_assets_time_complete.csv')\n",
    "\n",
    "print (deal_asset_time_order_filter.shape)\n",
    "print (list(deal_asset_time_order_filter.head(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1.5 Genenrate 'Date_to_Predict' for all assets without market transaction records.\n",
    "\n",
    "### Assume the 'Built' and 'Renovation' Month_Data to be (01-01) instead of (06-30) to reduce data quality issue.\n",
    "\n",
    "## Drop column 'Class' and 'MSACode' in 'Asset' Table because they cause some data quality issue.\n",
    "## Drop 'Amount' in 'Deals' table due to data quality issue.\n",
    "\n",
    "# Calculate the interval date from property 'Built' till the 'Prediction' date (12 months from now on)\n",
    "def date_to_predict(row):\n",
    "    return (pd.to_datetime(row['Prediction_Date']) - pd.to_datetime(row['Date_Built'])).days\n",
    "\n",
    "# Get the asset records without IA Transaction history.\n",
    "ia_not_assets = pd.read_excel(\"ia_not_assets.xlsx\")[['AssetID', 'City', 'State', 'CountryID', 'CityID', 'State.1', 'Country',\n",
    "                                         'Zip', 'PropertyTypeID', 'PropertyType', 'PropertyTypeGroup', 'YearBuilt', 'AssetLatitude',\n",
    "                                         'AssetLongitude', 'LandAreaInAcres', 'NumberofFloors', 'NumberofBuildings', 'YearRenovated',\n",
    "                                         'Size_Acres', 'Size_InLineSqft', 'IsPortfolio', 'PortfolioCount', 'Lat_city', 'Long_city',\n",
    "                                         'IsGateway', 'Region', 'Size_Sqft', 'Size_Units', 'PropertySize', 'UnitType', 'ClassDateStamp',\n",
    "                                         'Market_name', 'CityID.1']].drop_duplicates()\n",
    "\n",
    "# Column 'YearBuilt' is very important for modeling and analaysis.\n",
    "# Drop the rows with 'YearBuilt' value (null or <1700 or > 2019) \n",
    "# Drop the rows with 'YearRenovated' value (<1700 or > 2019)\n",
    "ia_not_assets = ia_not_assets [(ia_not_assets.YearBuilt.notnull()) & (ia_not_assets.YearBuilt < 2019) & (ia_not_assets.YearBuilt >= 1700)   & (ia_not_assets.YearRenovated.isnull() | ((ia_not_assets.YearRenovated < 2019) & (ia_not_assets.YearRenovated >= 1700)))]\n",
    "\n",
    "# Add columns with all 'NULL' value\n",
    "ia_not_assets[\"ClientCount\"] = np.NAN\n",
    "ia_not_assets['DealID'] = np.NAN\n",
    "ia_not_assets['AccountingDate'] = np.NAN\n",
    "ia_not_assets['BuyerSelectionDate'] = np.NAN\n",
    "ia_not_assets['InitialBidDueDate'] = np.NAN\n",
    "ia_not_assets['MarketingDate'] = np.NAN\n",
    "ia_not_assets['HireDate'] = np.NAN\n",
    "ia_not_assets['Deal_Start_Date'] = np.NAN\n",
    "ia_not_assets['Deal_End_Date'] = np.NAN\n",
    "\n",
    "# Calculate the value for columns 'Current_Date', 'Prediction_Date', 'Year_Built', 'Year_Renovated', 'Date_Built', 'Date_Renovated','Date_from_Built' and 'Date_from_Renovation'.\n",
    "ia_not_assets['Current_Date'] = pd.Timestamp(\"today\")\n",
    "ia_not_assets['Prediction_Date'] = pd.Timestamp(\"today\") + pd.DateOffset(years=1)\n",
    "ia_not_assets['Year_Built'] = pd.to_numeric(ia_not_assets['YearBuilt'], downcast='signed').astype(np.int)\n",
    "ia_not_assets['Year_Renovated'] = pd.to_numeric(np.where(ia_not_assets ['YearRenovated'].isnull(), -1, ia_not_assets['YearRenovated']), downcast='signed').astype(np.int)\n",
    "ia_not_assets['Date_Built'] = ia_not_assets.apply (lambda row: date_build(row),axis=1)\n",
    "ia_not_assets['Date_Renovated'] = ia_not_assets.apply (lambda row: date_renovation(row),axis=1)\n",
    "ia_not_assets['Date_from_Built'] = ia_not_assets.apply (lambda row: date_from_built(row),axis=1)\n",
    "ia_not_assets['Date_from_Renovation'] = ia_not_assets.apply (lambda row: date_from_renovation(row),axis=1)\n",
    "\n",
    "ia_not_assets['Interval_Deal_Start_Date'] = np.NAN\n",
    "ia_not_assets['Interval_Deal_End_Date'] = np.NAN\n",
    "\n",
    "# Calculate the value for columns 'Date_to_Market' and 'Date_to_Predict'\n",
    "ia_not_assets['Date_to_Market'] = ia_not_assets['Date_from_Built']\n",
    "ia_not_assets['Date_to_Predict'] = ia_not_assets.apply (lambda row: date_to_predict(row),axis=1)\n",
    "\n",
    "# Add the column \"Is_Transaction_Exist\", and set the value of each row to be 0 (Assets without market transaction).\n",
    "ia_not_assets[\"Is_Transaction_Exist\"] = 0\n",
    "\n",
    "# Sort each row in AssetID increasing order.\n",
    "ia_not_assets_order = ia_not_assets.sort_values(by = ['AssetID'])\n",
    "\n",
    "# Save the data frame to result file for further checking analysis purpose.\n",
    "ia_not_assets_order.to_csv('ia_not_assets_complete.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1.6 Concat the result data frame from 1.4 and 1.5 as the main data frame for modeling, analysis and prediction.\n",
    "\n",
    "total_asset_transactions = pd.concat([deal_asset_time_order_filter, ia_not_assets_order]).drop_duplicates()\n",
    "\n",
    "print (total_asset_transactions.shape)\n",
    "print (list(total_asset_transactions.head(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Part 2. Unsupervised learning and Data Visualization.\n",
    "####### The purpose is for feature Selection and dimension reduction. \n",
    "####### Figure out the most important features.\n",
    "\n",
    "\n",
    "##### 2.1 Please view the code and visualization parts of 'R' codes.\n",
    "##### Please check the code and visualization result in 'Lead_Generation.pdf'.\n",
    "##### Data source: 'ia_deal_assets_time_complete.csv' generated from the prior step. \n",
    "\n",
    "##### 2.2 Make further feature analysis, selection and dimension reduction with Information Gain.\n",
    "\n",
    "from info_gain import info_gain\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "### Calculate Information Gain. Drop the fields with too little Information Gain.\n",
    "\n",
    "# Other Optional Feature selection methods: Filtering, Wrapping, Gini-Gain.\n",
    "# Feature Transformation: PCA, ICA, Randomized Projection.\n",
    "# \"Date to Market\" can not be used to do information gain analysis\n",
    "# Of all 19 fields, remove the 5 fields with the least information gain for supervised learning: IsGateway, NumberofBuildings, PortfolioCount, IsPortfolio, and Number_of_Floors. \n",
    "\n",
    "# Identically and independently sample 10% of total data for dimension reduction analysis to avoid bias.\n",
    "sample_asset_transactions = total_asset_transactions.sample(frac=0.1, replace=False)\n",
    "\n",
    "print (sample_asset_transactions.shape)\n",
    "print (\"Information gain for the below columns:\")\n",
    "print (list(sample_asset_transactions.head(0)))\n",
    "\n",
    "# Set up two lists to save each field name and its Information Gain value.\n",
    "field_name = []\n",
    "information_gain = []\n",
    "\n",
    "ig_YearBuilt = info_gain.info_gain( sample_asset_transactions.Is_Transaction_Exist, sample_asset_transactions.YearBuilt)\n",
    "print (\"YearBuilt: \" + str(ig_YearBuilt))\n",
    "field_name.append(\"YearBuilt\")\n",
    "information_gain.append(ig_YearBuilt)\n",
    "\n",
    "ig_IsPortfolio = info_gain.info_gain(sample_asset_transactions.Is_Transaction_Exist, sample_asset_transactions.IsPortfolio)\n",
    "print(\"IsPortfolio: \" + str(ig_IsPortfolio))\n",
    "field_name.append(\"IsPortfolio\")\n",
    "information_gain.append(ig_IsPortfolio)\n",
    "\n",
    "ig_YearRenovated_NotNull = sample_asset_transactions [sample_asset_transactions.YearRenovated.notnull()]\n",
    "ig_YearRenovated  = info_gain.info_gain(ig_YearRenovated_NotNull.Is_Transaction_Exist, ig_YearRenovated_NotNull.YearRenovated)\n",
    "print (\"Year_Renovated: \" + str(ig_YearRenovated))\n",
    "field_name.append(\"Year_Renovated\")\n",
    "information_gain.append(ig_YearRenovated)\n",
    "\n",
    "ig_Date_from_Built = info_gain.info_gain( sample_asset_transactions.Is_Transaction_Exist, sample_asset_transactions.Date_from_Built)\n",
    "print (\"Date_from_Built: \" + str(ig_Date_from_Built))\n",
    "field_name.append(\"Date_from_Built\")\n",
    "information_gain.append(ig_Date_from_Built)\n",
    "\n",
    "ig_Date_from_Renovation_NotNull = sample_asset_transactions [(sample_asset_transactions.Date_from_Renovation.notnull()) & (sample_asset_transactions.Date_from_Renovation != -1)]\n",
    "ig_Date_from_Renovation = info_gain.info_gain(ig_Date_from_Renovation_NotNull.Is_Transaction_Exist, ig_Date_from_Renovation_NotNull.Date_from_Renovation)\n",
    "print (\"Date_from_Renovation: \" + str(ig_Date_from_Renovation))\n",
    "field_name.append(\"Date_from_Renovation\")\n",
    "information_gain.append(ig_Date_from_Renovation)\n",
    "\n",
    "ig_NumberofFloors_NotNull = sample_asset_transactions [sample_asset_transactions.Date_from_Renovation.notnull()]\n",
    "ig_NumberofFloors = info_gain.info_gain(ig_NumberofFloors_NotNull.Is_Transaction_Exist, ig_NumberofFloors_NotNull.NumberofFloors)\n",
    "print(\"Number_of_Floors: \" + str(ig_NumberofFloors))\n",
    "field_name.append(\"Number_of_Floors\")\n",
    "information_gain.append(ig_NumberofFloors)\n",
    "\n",
    "ig_NumberofBuildings_NotNull = sample_asset_transactions[sample_asset_transactions.NumberofBuildings.notnull()]\n",
    "ig_NumberofBuildings = info_gain.info_gain(ig_NumberofBuildings_NotNull.Is_Transaction_Exist, ig_NumberofBuildings_NotNull.NumberofBuildings)\n",
    "print(\"NumberofBuildings: \" + str(ig_NumberofBuildings))\n",
    "field_name.append(\"Number_of_Buildings\")\n",
    "information_gain.append(ig_NumberofBuildings)\n",
    "\n",
    "ig_IsGateway_NotNull = sample_asset_transactions[sample_asset_transactions.IsGateway.notnull()]\n",
    "ig_IsGateway = info_gain.info_gain(ig_IsGateway_NotNull.Is_Transaction_Exist, ig_IsGateway_NotNull.IsGateway)\n",
    "print(\"IsGateway: \" + str(ig_IsGateway))\n",
    "field_name.append(\"Is_Gateway\")\n",
    "information_gain.append(ig_IsGateway)\n",
    "\n",
    "ig_PortfolioCount_NotNull = sample_asset_transactions[sample_asset_transactions.PortfolioCount.notnull()]\n",
    "ig_PortfolioCount = info_gain.info_gain(ig_PortfolioCount_NotNull.Is_Transaction_Exist, ig_PortfolioCount_NotNull.PortfolioCount)\n",
    "print(\"PortfolioCount: \" + str(ig_PortfolioCount))\n",
    "field_name.append(\"Is_Portfolio_Count\")\n",
    "information_gain.append(ig_PortfolioCount)\n",
    "\n",
    "ig_Size_Units_NotNull = sample_asset_transactions[sample_asset_transactions.Size_Units.notnull()]\n",
    "ig_Size_Units = info_gain.info_gain(ig_Size_Units_NotNull.Is_Transaction_Exist, ig_Size_Units_NotNull.Size_Units)\n",
    "print(\"Size_Units: \" + str(ig_Size_Units))\n",
    "field_name.append(\"Size_Units\")\n",
    "information_gain.append(ig_Size_Units)\n",
    "\n",
    "ig_Size_Sqft_NotNull = sample_asset_transactions[sample_asset_transactions.Size_Sqft.notnull()]\n",
    "ig_Size_Sqft = info_gain.info_gain(ig_Size_Sqft_NotNull .Is_Transaction_Exist, ig_Size_Sqft_NotNull.Size_Sqft)\n",
    "print(\"Size_Sqft: \" + str(ig_Size_Sqft))\n",
    "field_name.append(\"Size_Sqft\")\n",
    "information_gain.append(ig_Size_Sqft)\n",
    "\n",
    "ig_LandAreaInAcres_NotNull = sample_asset_transactions[sample_asset_transactions.LandAreaInAcres.notnull()]\n",
    "ig_LandAreaInAcres = info_gain.info_gain(ig_LandAreaInAcres_NotNull.Is_Transaction_Exist, ig_LandAreaInAcres_NotNull.LandAreaInAcres)\n",
    "print(\"LandAreaInAcres: \" + str(ig_LandAreaInAcres))\n",
    "field_name.append(\"Land_Area_InAcres\")\n",
    "information_gain.append(ig_LandAreaInAcres)\n",
    "\n",
    "ig_Size_Acres_NotNull = sample_asset_transactions[sample_asset_transactions.Size_Acres.notnull()]\n",
    "ig_Size_Acres = info_gain.info_gain(ig_Size_Acres_NotNull.Is_Transaction_Exist, ig_Size_Acres_NotNull.Size_Acres)\n",
    "print(\"Size_Acres: \" + str(ig_Size_Acres))\n",
    "field_name.append(\"Size_Acres\")\n",
    "information_gain.append(ig_Size_Acres)\n",
    "\n",
    "ig_Size_InLineSqft_NotNull = sample_asset_transactions[sample_asset_transactions.Size_InLineSqft.notnull()]\n",
    "ig_Size_InLineSqft = info_gain.info_gain(ig_Size_InLineSqft_NotNull.Is_Transaction_Exist, ig_Size_InLineSqft_NotNull.Size_InLineSqft)\n",
    "print(\"Size_InLineSqft: \" + str(ig_Size_InLineSqft))\n",
    "field_name.append(\"Size_InLineSqft\")\n",
    "information_gain.append(ig_Size_InLineSqft)\n",
    "\n",
    "ig_AssetLatitude_NotNull = sample_asset_transactions[sample_asset_transactions.AssetLatitude.notnull()]\n",
    "ig_AssetLatitude  = info_gain.info_gain(ig_AssetLatitude_NotNull.Is_Transaction_Exist, ig_AssetLatitude_NotNull.AssetLatitude)\n",
    "print(\"AssetLatitude: \"  + str(ig_AssetLatitude))\n",
    "field_name.append(\"Asset_Latitude\")\n",
    "information_gain.append(ig_AssetLatitude)\n",
    "\n",
    "ig_AssetLongitude_NotNull = sample_asset_transactions[sample_asset_transactions.AssetLongitude.notnull()]\n",
    "ig_AssetLongitude = info_gain.info_gain(ig_AssetLongitude_NotNull.Is_Transaction_Exist, ig_AssetLongitude_NotNull.AssetLongitude)\n",
    "print(\"AssetLongitude: \" + str(ig_AssetLongitude))\n",
    "field_name.append(\"Asset_Longitude\")\n",
    "information_gain.append(ig_AssetLongitude)\n",
    "\n",
    "ig_Lat_city_NotNull = sample_asset_transactions[sample_asset_transactions.Lat_city.notnull()]\n",
    "ig_Lat_city = info_gain.info_gain(ig_Lat_city_NotNull.Is_Transaction_Exist, ig_Lat_city_NotNull.Lat_city)\n",
    "print(\"Lat_city: \" + str(ig_Lat_city))\n",
    "field_name.append(\"Lat_city\")\n",
    "information_gain.append(ig_Lat_city)\n",
    "\n",
    "ig_Long_city_NotNull = sample_asset_transactions[sample_asset_transactions.Long_city.notnull()]\n",
    "ig_Long_city = info_gain.info_gain(ig_Long_city_NotNull.Is_Transaction_Exist, ig_Long_city_NotNull.Long_city)\n",
    "print(\"Long_city: \" + str(ig_Long_city))\n",
    "field_name.append(\"Long_city\")\n",
    "information_gain.append(ig_Long_city)\n",
    "\n",
    "# Form a data frame to save the Information Gain value with its relevant field name.\n",
    "ig_frames = pd.DataFrame({\n",
    "    'field_name' : field_name,\n",
    "    'information_gain' : information_gain })\n",
    "\n",
    "# Sort each field name by the Information Gain decreasing order.\n",
    "ig_frames_sort = ig_frames.sort_values(by =['information_gain'], ascending=False)\n",
    "\n",
    "# Generate the figure to show each field in Information Gain decreasing order.\n",
    "plt.bar(ig_frames_sort.field_name, ig_frames_sort.information_gain,  align='center', alpha=0.5)\n",
    "plt.xlabel('Field Name')\n",
    "plt.xticks(ig_frames_sort.field_name, rotation=90 )\n",
    "plt.ylabel('Information Gain Value')\n",
    "plt.title('Information Gain Value for Different Fields')\n",
    "plt.savefig('information_gain.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2.3 Add all required columns for Statistical / Machine Learning modeling and analysis purpose. \n",
    "total_asset_transactions[\"Logistic_Regression\"] = np.NAN\n",
    "total_asset_transactions[\"Logistic_Regression_Accuracy\"] = np.NAN\n",
    "total_asset_transactions[\"Logistic_Regression_Accuracy_1\"] = np.NAN\n",
    "total_asset_transactions[\"Decision_Tree\"] = np.NAN\n",
    "total_asset_transactions[\"Decision_Tree_Accuracy\"] = np.NAN\n",
    "total_asset_transactions[\"Decision_Tree_Accuracy_1\"] = np.NAN\n",
    "total_asset_transactions[\"Neural_Network\"] = np.NAN\n",
    "total_asset_transactions[\"Neural_Network_Accuracy\"] = np.NAN\n",
    "total_asset_transactions[\"Neural_Network_Accuracy_1\"] = np.NAN\n",
    "total_asset_transactions[\"Support_Vector_Machine\"] = np.NAN\n",
    "total_asset_transactions[\"SVM_Accuracy\"] = np.NAN\n",
    "total_asset_transactions[\"SVM_Accuracy_1\"] = np.NAN\n",
    "total_asset_transactions[\"Boosting\"] = np.NAN\n",
    "total_asset_transactions[\"Boosting_Accuracy\"] = np.NAN\n",
    "total_asset_transactions[\"Boosting_Accuracy_1\"] = np.NAN\n",
    "\n",
    "total_asset_transactions[\"Bi_Normal_Distribution_Probability\"] = np.NAN\n",
    "total_asset_transactions[\"Weibull_Distribution_Probability\"] = np.NAN\n",
    "\n",
    "total_asset_transactions[\"Market_Probability\"] = np.NAN\n",
    "total_asset_transactions[\"Recommendation_Level\"] = np.NAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Part 3. Build Machine Learning and Statistical models. Make analysis and predictions.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "##### 3.1 Identically and independently sample 10% of total data for dimension reduction analysis to avoid bias.\n",
    "\n",
    "### 'sample_asset_transactions' is generated in Part 2, and contains 10% of random data.\n",
    "###  Select the important attribuates for learning.\n",
    "sample_asset_transactions_0 = sample_asset_transactions [['Date_from_Built', 'Date_from_Renovation', 'Size_Sqft', 'Size_Acres', 'Size_Units', 'LandAreaInAcres',\n",
    "                                                          'PropertyTypeID','NumberofBuildings', 'AssetLatitude', 'AssetLongitude', 'Lat_city', 'Long_city',\n",
    "                                                          'IsPortfolio', 'PortfolioCount', 'IsGateway', 'Date_to_Market', 'Is_Transaction_Exist']].fillna(0)\n",
    "\n",
    "# Divides the sample data to 80% training and 20% testing randomly.\n",
    "msk = np.random.rand(len(sample_asset_transactions_0)) < 0.8\n",
    "SL_Train = sample_asset_transactions_0[msk]\n",
    "SL_Test = sample_asset_transactions_0[~msk]\n",
    "\n",
    "# Get the training features and labels\n",
    "SL_Train_X = SL_Train.iloc[:, :-1]\n",
    "SL_Train_y = SL_Train.iloc[:, -1]\n",
    "\n",
    "# Get the testing features and labels\n",
    "SL_Test_X = SL_Test.iloc[:, :-1]\n",
    "SL_Test_y = SL_Test.iloc[:, -1]\n",
    "\n",
    "### Set up multiple lists to store the result data.\n",
    "model_names = []\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "test_recall_list = []\n",
    "test_precision_list = []\n",
    "model_std_errors = []\n",
    "train_times = []\n",
    "prediction_times = []\n",
    "\n",
    "##### 3.2 Build Logistic Regression models\n",
    "\n",
    "### Show the logistic regression theory.\n",
    "### Suitable for the vector with lots of 'Null' value.\n",
    "### Simple Statistical Theory (Easy to understand.)\n",
    "### Higher prediction speed (Suitable for industry need.)\n",
    "\n",
    "### To do list: select multiple different column combination to do training.\n",
    "### Generate Learning Curve to show the learning process and guard against overfitting.\n",
    "### Use 10-fold cross validation to improve accuracy\n",
    "### Use Testing accuracy as the final accuracy  Improvement: Use the Cross Validation accuracy.\n",
    "### Use 'Confusion Matrix' to evaluate accuracy to find improvement: 'Precision', 'Recall' and 'Accuracy'\n",
    "\n",
    "# Train model\n",
    "start_time = dt.datetime.now()\n",
    "\n",
    "Logistic_model = LogisticRegression()\n",
    "Logistic_model.fit(SL_Train_X, SL_Train_y)\n",
    "\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Get the training and testing accuracy\n",
    "predicted_train_classes = Logistic_model.predict(SL_Train_X)\n",
    "predicted_test_classes = Logistic_model.predict(SL_Test_X)\n",
    "\n",
    "train_accuracy = accuracy_score(SL_Train_y, predicted_train_classes)\n",
    "test_accuracy = accuracy_score(SL_Test_y, predicted_test_classes)\n",
    "test_recall = recall_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "test_precision = precision_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "\n",
    "model_names.append (\"Logistic_Regression\")\n",
    "train_accuracy_list.append (train_accuracy)\n",
    "test_accuracy_list.append (test_accuracy) \n",
    "test_recall_list.append (test_recall)\n",
    "test_precision_list.append(test_precision)\n",
    "model_std_errors.append(np.NAN)\n",
    "\n",
    "parameters = Logistic_model.coef_\n",
    "\n",
    "print (train_accuracy)\n",
    "print (test_accuracy) # Most important \n",
    "print (test_recall)\n",
    "print (test_precision)\n",
    "print (parameters)\n",
    "print (confusion_matrix(SL_Test_y, predicted_test_classes))\n",
    "print (classification_report(SL_Test_y, predicted_test_classes))\n",
    "\n",
    "# 0.9973963009516968\n",
    "# 0.9967509025270758\n",
    "# 0.9759358288770054\n",
    "# 0.9982638888888888\n",
    "# [[ 2.63335027e-02  2.19423333e-05 -6.71914521e-08 -4.25657326e-04\n",
    "#   -3.87426307e-04 -2.54802891e-03 -4.03197853e-03 -1.26625430e-05\n",
    "#   -8.54026471e-03  2.15459471e-02 -6.66240928e-03  1.74716598e-02\n",
    "#   -4.19825292e-05 -4.53191319e-05 -4.08402082e-05 -2.64754610e-02]]\n",
    "# [[2583    0]\n",
    "#  [   9  178]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00      2583\n",
    "#            1       1.00      0.95      0.98       187\n",
    "\n",
    "#    micro avg       1.00      1.00      1.00      2770\n",
    "#    macro avg       1.00      0.98      0.99      2770\n",
    "# weighted avg       1.00      1.00      1.00      2770\n",
    "\n",
    "# Make the predictions\n",
    "asset_transactions_full = total_asset_transactions [['Date_from_Built', 'Date_from_Renovation', 'Size_Sqft', 'Size_Acres', 'Size_Units', 'LandAreaInAcres',\n",
    "                                                          'PropertyTypeID', 'NumberofBuildings', 'AssetLatitude', 'AssetLongitude', 'Lat_city', 'Long_city',\n",
    "                                                          'IsPortfolio', 'PortfolioCount', 'IsGateway', 'Date_to_Predict']].fillna(0)\n",
    "\n",
    "start_time = dt.datetime.now()\n",
    "total_asset_transactions[\"Logistic_Regression\"] = Logistic_model.predict(asset_transactions_full)\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Save the prediction result.\n",
    "total_asset_transactions[\"Logistic_Regression_Accuracy\"] = test_accuracy\n",
    "total_asset_transactions[\"Logistic_Regression_Accuracy_1\"] = test_accuracy\n",
    "total_asset_transactions[\"Logistic_Regression_Accuracy_1\"] = np.where(total_asset_transactions[\"Logistic_Regression\"] == 1, total_asset_transactions[\"Logistic_Regression_Accuracy\"], 1 - total_asset_transactions[\"Logistic_Regression_Accuracy\"])\n",
    "\n",
    "### Coefficient is reasonable, since there is not much large, prevent overfitting,\n",
    "### Also, Date_to_Market has almost the largest coefficient, indicating that it is most important.\n",
    "### Model Issue: The more transaction a property has, the shorter it will be into the market next time.\n",
    "### The longer time it does not have transaciton, it will not be easiliy into the market.  (May not totally agree to our basic instinct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3.3 Build Decision Tree models\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Do not use too much fields, make the tree not to be too complex, otherwise, it may overfitting.\n",
    "# Decision tree pruning is necessary sometimes to reduce ovefitting.\n",
    "# Tune the value of pruning, max no. of instance per nodes (small value can reduce complexity), confidence factor (small value means more pruning), subtree lifting, etc.\n",
    "\n",
    "# Train model\n",
    "start_time = dt.datetime.now()\n",
    "\n",
    "Decision_Tree = DecisionTreeClassifier()\n",
    "Decision_Tree.fit(SL_Train_X, SL_Train_y)\n",
    "\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Get the training and testing accuracy\n",
    "predicted_train_classes = Decision_Tree.predict(SL_Train_X)\n",
    "predicted_test_classes = Decision_Tree.predict(SL_Test_X)\n",
    "train_accuracy = accuracy_score(SL_Train_y, predicted_train_classes)\n",
    "test_accuracy = accuracy_score(SL_Test_y, predicted_test_classes)\n",
    "test_recall = recall_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "test_precision = precision_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "\n",
    "model_names.append (\"Decision_Tree\")\n",
    "train_accuracy_list.append (train_accuracy)\n",
    "test_accuracy_list.append (test_accuracy) \n",
    "test_recall_list.append (test_recall)\n",
    "test_precision_list.append(test_precision)\n",
    "model_std_errors.append(np.NAN)\n",
    "\n",
    "tree.export_graphviz(Decision_Tree, out_file='decision_tree.dot')\n",
    "\n",
    "print (train_accuracy)\n",
    "print (test_accuracy)\n",
    "print (test_recall)\n",
    "print (test_precision)\n",
    "print (confusion_matrix(SL_Test_y, predicted_test_classes))\n",
    "print (classification_report(SL_Test_y, predicted_test_classes))\n",
    "\n",
    "# 1.0   (Overfitting happens!)\n",
    "# 0.9379061371841155\n",
    "# 0.760846836887009\n",
    "# 0.7533260549178564\n",
    "# [[2494   89]\n",
    "#  [  83  104]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.97      0.97      0.97      2583\n",
    "#            1       0.54      0.56      0.55       187\n",
    "\n",
    "#    micro avg       0.94      0.94      0.94      2770\n",
    "#    macro avg       0.75      0.76      0.76      2770\n",
    "# weighted avg       0.94      0.94      0.94      2770\n",
    "\n",
    "# Make the predictions\n",
    "start_time = dt.datetime.now()\n",
    "total_asset_transactions[\"Decision_Tree\"] = Decision_Tree.predict(asset_transactions_full)\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Save the prediction result.\n",
    "total_asset_transactions[\"Decision_Tree_Accuracy\"] = test_accuracy\n",
    "total_asset_transactions[\"Decision_Tree_Accuracy_1\"] = test_accuracy\n",
    "total_asset_transactions[\"Decision_Tree_Accuracy_1\"] = np.where(total_asset_transactions[\"Decision_Tree\"] == 1, total_asset_transactions[\"Decision_Tree_Accuracy\"], 1 - total_asset_transactions[\"Decision_Tree_Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3.4 Build Neural Network (Multiple Layer Perceptron) models\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier  # Multiple Layer Perceptron\n",
    "\n",
    "### Default layer to be 1, the hidden nodes are using the formula (dimensions + class)/2 = 8\n",
    "### Do not use too much fields, make the network not to be too complex, otherwise, it may overfit.\n",
    "### Tune the value of hidden layer, no. of nodes each layer, max training iterations, etc.\n",
    "\n",
    "# Issue: The more transaction a property has, the shorter it will be into the market next time.\n",
    "# The longer time it does not have transaciton, it will not be easiliy into the market.  (May not totally agree to our basic instinct)\n",
    "\n",
    "# Train model\n",
    "start_time = dt.datetime.now()\n",
    "Neural_Network_Classifier = MLPClassifier(hidden_layer_sizes=(8, ), max_iter=1000, momentum=0.3)\n",
    "\n",
    "# Default: MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)[source]¶\n",
    "Neural_Network_Classifier.fit(SL_Train_X, SL_Train_y)\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Get the training and testing accuracy\n",
    "predicted_train_classes = Neural_Network_Classifier.predict(SL_Train_X)\n",
    "predicted_test_classes = Neural_Network_Classifier.predict(SL_Test_X)\n",
    "train_accuracy = accuracy_score(SL_Train_y, predicted_train_classes)\n",
    "test_accuracy = accuracy_score(SL_Test_y, predicted_test_classes)\n",
    "test_recall = recall_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "test_precision = precision_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "\n",
    "model_names.append (\"Neural_Network\")\n",
    "train_accuracy_list.append (train_accuracy)\n",
    "test_accuracy_list.append (test_accuracy) \n",
    "test_recall_list.append (test_recall)\n",
    "test_precision_list.append(test_precision)\n",
    "model_std_errors.append(np.NAN)\n",
    "\n",
    "print (train_accuracy)\n",
    "print (test_accuracy)\n",
    "print (test_recall)\n",
    "print (test_precision)\n",
    "print (confusion_matrix(SL_Test_y, predicted_test_classes))\n",
    "print (classification_report(SL_Test_y, predicted_test_classes))\n",
    "\n",
    "# 0.9829412820973245\n",
    "# 0.9844765342960289\n",
    "# 0.8949476316764695\n",
    "# 0.9793936713441358\n",
    "# [[2579    4]\n",
    "#  [  39  148]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      1.00      0.99      2583\n",
    "#            1       0.97      0.79      0.87       187\n",
    "\n",
    "#    micro avg       0.98      0.98      0.98      2770\n",
    "#    macro avg       0.98      0.89      0.93      2770\n",
    "# weighted avg       0.98      0.98      0.98      2770\n",
    "\n",
    "# Make the predictions\n",
    "start_time = dt.datetime.now()\n",
    "total_asset_transactions[\"Neural_Network\"] = Neural_Network_Classifier.predict(asset_transactions_full)\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Save the prediciton result\n",
    "total_asset_transactions[\"Neural_Network_Accuracy\"] = test_accuracy\n",
    "total_asset_transactions[\"Neural_Network_Accuracy_1\"] = test_accuracy\n",
    "total_asset_transactions[\"Neural_Network_Accuracy_1\"] = np.where(total_asset_transactions[\"Neural_Network\"] == 1, total_asset_transactions[\"Decision_Tree_Accuracy\"], 1 - total_asset_transactions[\"Neural_Network_Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3.5 Build Support Vector Machine models\n",
    "\n",
    "### To do: Change Kernal methods: RBF (Radial Basis Function); Polynomial\n",
    "### Linear kernal function performs comparably to Logistic Regression in practice. However, SVM may perform better due to its maximum margin and optimal hyperplane,\n",
    "### SVM is less sensitive to outliers.\n",
    "### SVM is resilient to overfitting.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train model\n",
    "start_time = dt.datetime.now()\n",
    "SVM_classifier = SVC(kernel='linear')\n",
    "SVM_classifier.fit(SL_Train_X, SL_Train_y)\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Get the training and testing accuracy\n",
    "predicted_train_classes = SVM_classifier.predict(SL_Train_X)\n",
    "predicted_test_classes = SVM_classifier.predict(SL_Test_X)\n",
    "train_accuracy = accuracy_score(SL_Train_y, predicted_train_classes)\n",
    "test_accuracy = accuracy_score(SL_Test_y, predicted_test_classes)\n",
    "test_recall = recall_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "test_precision = precision_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "\n",
    "model_names.append (\"Support_Vector_Machine\")\n",
    "train_accuracy_list.append (train_accuracy)\n",
    "test_accuracy_list.append (test_accuracy) \n",
    "test_recall_list.append (test_recall)\n",
    "test_precision_list.append(test_precision)\n",
    "model_std_errors.append(np.NAN)\n",
    "\n",
    "print (train_accuracy)\n",
    "print (test_accuracy)\n",
    "print (test_recall)\n",
    "print (test_precision)\n",
    "print (confusion_matrix(SL_Test_y, predicted_test_classes))\n",
    "print (classification_report(SL_Test_y, predicted_test_classes))\n",
    "\n",
    "# 1.0\n",
    "# 1.0\n",
    "# 1.0\n",
    "# 1.0\n",
    "# [[2583    0]\n",
    "#  [   0  187]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00      2583\n",
    "#            1       1.00      1.00      1.00       187\n",
    "\n",
    "#    micro avg       1.00      1.00      1.00      2770\n",
    "#    macro avg       1.00      1.00      1.00      2770\n",
    "# weighted avg       1.00      1.00      1.00      2770\n",
    "\n",
    "\n",
    "# Make the predictions\n",
    "start_time = dt.datetime.now()\n",
    "total_asset_transactions[\"Support_Vector_Machine\"] = SVM_classifier.predict(asset_transactions_full)\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Save the prediciton result\n",
    "total_asset_transactions[\"SVM_Accuracy\"] = test_accuracy\n",
    "total_asset_transactions[\"SVM_Accuracy_1\"] = test_accuracy\n",
    "total_asset_transactions[\"SVM_Accuracy_1\"] = np.where(total_asset_transactions[\"Support_Vector_Machine\"] == 1, total_asset_transactions[\"SVM_Accuracy\"], 1 - total_asset_transactions[\"SVM_Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3.6 Build Boosting model - Ada Boost\n",
    "\n",
    "### Boosting: Multiple weak classifers work together to determine the classifier output. Use Ada Boost.\n",
    "### Higher accuracy due to expanded decision hyperplan.\n",
    "### It could normally take much longer time to train.\n",
    "### The next weak classifier is based on the trained weak classifier, and mistrained instance will have higher weight.\n",
    "### To do: Change weak classifiers, and the parameter of each weak classifier.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Train model\n",
    "seed = 10\n",
    "num_trees = 20\n",
    "start_time = dt.datetime.now()\n",
    "Boosting_Model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "Boosting_Model.fit(SL_Train_X, SL_Train_y)\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Get the training and testing accuracy\n",
    "predicted_train_classes = Boosting_Model.predict(SL_Train_X)\n",
    "predicted_test_classes = Boosting_Model.predict(SL_Test_X)\n",
    "train_accuracy = accuracy_score(SL_Train_y, predicted_train_classes)\n",
    "test_accuracy = accuracy_score(SL_Test_y, predicted_test_classes)\n",
    "test_recall = recall_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "test_precision = precision_score(SL_Test_y, predicted_test_classes, average='macro')\n",
    "\n",
    "model_names.append (\"Boosting\")\n",
    "train_accuracy_list.append (train_accuracy)\n",
    "test_accuracy_list.append (test_accuracy) \n",
    "test_recall_list.append (test_recall)\n",
    "test_precision_list.append(test_precision)\n",
    "model_std_errors.append(np.NAN)\n",
    "\n",
    "print (train_accuracy)\n",
    "print (test_accuracy)\n",
    "print (test_recall)\n",
    "print (test_precision)\n",
    "print(confusion_matrix(SL_Test_y, predicted_test_classes))\n",
    "print(classification_report(SL_Test_y, predicted_test_classes))\n",
    "\n",
    "# 0.9478362363081343\n",
    "# 0.9447653429602888\n",
    "# 0.600829984617646\n",
    "# 0.9250715682167295\n",
    "# [[2579    4]\n",
    "#  [ 149   38]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.95      1.00      0.97      2583\n",
    "#            1       0.90      0.20      0.33       187\n",
    "\n",
    "#    micro avg       0.94      0.94      0.94      2770\n",
    "#    macro avg       0.93      0.60      0.65      2770\n",
    "# weighted avg       0.94      0.94      0.93      2770\n",
    "\n",
    "# Make the predictions\n",
    "start_time = dt.datetime.now()\n",
    "total_asset_transactions[\"Boosting\"] = Boosting_Model.predict(asset_transactions_full)\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "# Save the prediciton result\n",
    "total_asset_transactions[\"Boosting_Accuracy\"] = test_accuracy\n",
    "total_asset_transactions[\"Boosting_Accuracy_1\"] = test_accuracy\n",
    "total_asset_transactions[\"Boosting_Accuracy_1\"] = np.where(total_asset_transactions[\"Boosting\"] == 1, total_asset_transactions[\"Boosting_Accuracy\"], 1 - total_asset_transactions[\"Boosting_Accuracy\"])\n",
    "\n",
    "#####################################################################\n",
    "### Several classifiers are not selected as below:\n",
    "### KNN is not selected due to the difficulty of gaining distance for each dimension, how single dimension distance plays the role in the final distance. Also, avoid curse of dimensionality.\n",
    "### HMM: Unknown Transition, Imition probability.\n",
    "### Naive Bayes: Condition Independent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 3.7 Build Statistical Modeling - Linear Regression\n",
    "\n",
    "### Based on the data analysis and visualization, 'Date_from_Built' and 'Date_to_Market' has strong linear relationship.\n",
    "### Calculate the Standard error of this linear regression\n",
    "### Change the area (region, state, city, etc) for the modeling of specific areas.\n",
    "### Add more measurement factors in the linear regression model.\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Filter the transaction record based on 'Date_to_Market' values.\n",
    "deal_asset_time_order_filter_new = deal_asset_time_order_filter[deal_asset_time_order_filter.Date_to_Market >= 90]\n",
    "\n",
    "# Initiate Linear regression model\n",
    "start_time = dt.datetime.now()\n",
    "Linear_Regression = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "Linear_Regression.fit(deal_asset_time_order_filter_new[['Date_from_Built']], deal_asset_time_order_filter_new['Date_to_Market'])\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "print (Linear_Regression.coef_)\n",
    "print (Linear_Regression.intercept_)\n",
    "\n",
    "# [0.9095816]\n",
    "# -1391.8770039558985\n",
    "\n",
    "# Predict 'Date_to_Market' based on the linear regression model (for training dataset)\n",
    "Date_Linear_Predict = Linear_Regression.predict(deal_asset_time_order_filter_new[['Date_from_Built']])\n",
    "\n",
    "# Get the standard deviation of the linear regression model (for training dataset)\n",
    "std_error = np.sqrt(mean_squared_error(Date_Linear_Predict, deal_asset_time_order_filter_new['Date_to_Market']))\n",
    "r2_coefficient = r2_score(Date_Linear_Predict, deal_asset_time_order_filter_new['Date_to_Market'])\n",
    "\n",
    "print ( \"Prediction Standard Error: \" + str(std_error))\n",
    "print (\"R2 Coefficient of Determination: \"+ str(r2_coefficient))\n",
    "\n",
    "# Prediction Standard Error: 3438.454228361336\n",
    "# R2 Coefficient of Determination: 0.7739593610255957\n",
    "\n",
    "plt.scatter(deal_asset_time_order_filter_new['Date_from_Built'], deal_asset_time_order_filter_new['Date_to_Market'])\n",
    "plt.plot(deal_asset_time_order_filter_new['Date_from_Built'], Date_Linear_Predict)\n",
    "plt.xlabel('Date to Built')\n",
    "plt.ylabel('Date to Market')\n",
    "plt.title('Linear Regression for Date to Market vs Date to Built')\n",
    "plt.savefig('linear_regression.png')\n",
    "plt.show()\n",
    "\n",
    "model_names.append (\"Linear_Regression\")\n",
    "model_std_errors.append(std_error)\n",
    "train_accuracy_list.append (np.NAN)\n",
    "test_accuracy_list.append (np.NAN) \n",
    "test_recall_list.append (np.NAN)\n",
    "test_precision_list.append(np.NAN)\n",
    "    \n",
    "start_time = dt.datetime.now()\n",
    "\n",
    "# Make 'Date_to_Market' predictions for the whole data set\n",
    "total_Date_Linear_Predict = Linear_Regression.predict(total_asset_transactions[['Date_from_Built']])\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "# Predict and save the market probability based on the 'Date_to_Predict' column\n",
    "total_asset_transactions[\"Linear_Regression_Probability\"] = st.norm.cdf((total_asset_transactions['Date_to_Predict'] - total_Date_Linear_Predict) / std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 3.8 Build Statistical Modeling - Bi-modal Normal distribution.\n",
    "\n",
    "##### Use the combination of 2 normal distribution, based on the Bi-modal normal distribution of 'Date_to_Market' curve.\n",
    "\n",
    "### Calculate the standard error of 'Date_to_Market'\n",
    "start_time = dt.datetime.now()\n",
    "norm_std_error = np.std(deal_asset_time_order_filter_new['Date_to_Market'])\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "model_names.append (\"Bi-modal_Normal_Distribution\")\n",
    "model_std_errors.append(norm_std_error)\n",
    "train_accuracy_list.append (np.NAN)\n",
    "test_accuracy_list.append (np.NAN) \n",
    "test_recall_list.append (np.NAN)\n",
    "test_precision_list.append(np.NAN)\n",
    "\n",
    "# print (norm_std_error)\n",
    "# 8007\n",
    "\n",
    "### Predict and save the market probability based on the 'Date_to_Predict' column \n",
    "start_time = dt.datetime.now()\n",
    "total_asset_transactions[\"Bi_Normal_Distribution_Probability\"]= 0.5 * (st.norm(1500, norm_std_error).cdf(total_asset_transactions['Date_to_Predict'] ) + st.norm(11000, norm_std_error).cdf(total_asset_transactions['Date_to_Predict'] ))\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(deal_asset_time_order_filter_new[['Date_to_Market']])\n",
    "m1, m2 = clf.means_\n",
    "w1, w2 = clf.weights_\n",
    "c1, c2 = clf.covariances_ \n",
    "\n",
    "print (m1)\n",
    "print (m2)\n",
    "print (w1)\n",
    "print (w2)\n",
    "print (c1)\n",
    "print (c2)\n",
    "\n",
    "# [20530.77761653]\n",
    "# [7124.37071675]\n",
    "# 0.14143008521606848\n",
    "# 0.8585699147839324\n",
    "# [[1.41457215e+08]]\n",
    "# [[25969782.696412]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 3.9 Build Statistical Modeling - Weibull distribution.\n",
    "### Weibull distribution to model 'Date_to_Market'.\n",
    "### Use the linear combination of 'Date_from_Built', 'Date_from_Renovation' and 'PortfolioCount' as the beta (shape) value.\n",
    "\n",
    "### Calculate the beta (shape), location, and eta (scale) value.\n",
    "start_time = dt.datetime.now()\n",
    "beta, location, eta = st.weibull_min.fit(deal_asset_time_order_filter_new['Date_to_Market'].astype(np.float64))\n",
    "train_times.append(dt.datetime.now() - start_time)\n",
    "print (beta)\n",
    "print (location)\n",
    "print (eta)\n",
    "\n",
    "std_error = np.sqrt(st.weibull_min.std(c= beta, loc=location, scale=eta))\n",
    "model_names.append (\"Weibull_Distribution\")\n",
    "model_std_errors.append(std_error)\n",
    "train_accuracy_list.append (np.NAN)\n",
    "test_accuracy_list.append (np.NAN) \n",
    "test_recall_list.append (np.NAN)\n",
    "test_precision_list.append(np.NAN)\n",
    "\n",
    "print (std_error)\n",
    "\n",
    "# 0.10451769749916562\n",
    "# 89.99999999999997\n",
    "# 2.5503610789148983\n",
    "# 32986.09534077522\n",
    "\n",
    "### Set up Linear Regression to calculate the 3 coefficients. \n",
    "linear_fit = linear_model.LinearRegression()          \n",
    "linear_fit.fit(deal_asset_time_order_filter_new[['Date_from_Built', 'Date_from_Renovation', 'PortfolioCount']].fillna(0), eta * np.ones(deal_asset_time_order_filter_new.shape[0]))\n",
    "\n",
    "### Predict and save the 'Date_to_Market' probability\n",
    "eta_factor_records = total_asset_transactions[['Date_from_Built', 'Date_from_Renovation', 'PortfolioCount']].fillna(0)\n",
    "\n",
    "start_time = dt.datetime.now()\n",
    "total_asset_transactions['eta_predict'] = linear_fit.predict(eta_factor_records[['Date_from_Built', 'Date_from_Renovation', 'PortfolioCount']])\n",
    "prediction_times.append(dt.datetime.now() - start_time)\n",
    "\n",
    "total_asset_transactions[\"Weibull_Distribution_Probability\"] = st.weibull_min(c=beta, loc=location, scale=total_asset_transactions[['eta_predict']]).cdf(total_asset_transactions[[\"Date_to_Predict\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the image to compare the accuracy, precision, recall of the above 5 Machine Learning algorithms.\n",
    "### Testing accuracy is most important.\n",
    "\n",
    "# Form a data frame to save the Information Gain value with its relevant field name.\n",
    "model_performance_frames = pd.DataFrame({\n",
    "    'model_name' : model_names,\n",
    "    'train_accuracy' : train_accuracy_list,\n",
    "    'test_accuracy' : test_accuracy_list,\n",
    "    'test_recall' : test_recall_list,\n",
    "    'test_precision' : test_precision_list,\n",
    "    'standard_error' : model_std_errors})\n",
    "\n",
    "# Sort each row of Machine Learning models by the Testing Accuracy decreasing order.\n",
    "ml_model_performance_sort = model_performance_frames.loc[0:4].sort_values(by =['test_accuracy'], ascending=False)\n",
    "print (ml_model_performance_sort)\n",
    "\n",
    "ml_model_performance_sort.plot(x = \"model_name\", y=[\"test_accuracy\", \"train_accuracy\", \"test_recall\", \"test_precision\"], kind=\"bar\")\n",
    "plt.savefig('ml_model_performance.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort each row of Statistical models by the Testing Accuracy decreasing order.\n",
    "statistic_model_performance_sort = model_performance_frames.loc[5:8].sort_values(by =['standard_error'], ascending=True)\n",
    "print (statistic_model_performance_sort)\n",
    "\n",
    "# Generate the figure to show the performance to each statistical model by the standard error increasing order.\n",
    "plt.bar(statistic_model_performance_sort.model_name, statistic_model_performance_sort.standard_error,  align='center', alpha=0.5)\n",
    "plt.xlabel('Model Name')\n",
    "plt.xticks(statistic_model_performance_sort.model_name, rotation=30 )\n",
    "plt.ylabel('Standard Error')\n",
    "plt.title('Statistical Model Performance Comparison')\n",
    "plt.savefig('statistic_model_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the figure to show the training and prediction time performance for each model\n",
    "\n",
    "# Form a data frame to save the train and prediction time performance with its relevant model name.\n",
    "\n",
    "time_performance_frames = pd.DataFrame({\n",
    "    'model_name' : model_names,\n",
    "    'train_time' : train_times,\n",
    "    'prediction_time' : prediction_times})\n",
    "\n",
    "# Sort each row of model by prediciton time ascending order.\n",
    "time_performance_frames_sort_0 = time_performance_frames.sort_values(by = ['prediction_time'], ascending=True)\n",
    "print (time_performance_frames_sort_0)\n",
    "\n",
    "time_performance_frames_sort_0.plot(x = \"model_name\", y=[\"prediction_time\"], kind=\"bar\")\n",
    "plt.savefig('model_prediction_time_performance.png')\n",
    "plt.show()\n",
    "\n",
    "# Sort each row of model by training time ascending order.\n",
    "time_performance_frames_sort_1 = time_performance_frames.sort_values(by = ['train_time'], ascending=True)\n",
    "print (time_performance_frames_sort_1)\n",
    "\n",
    "time_performance_frames_sort_1.plot(x = \"model_name\", y=[\"train_time\"], kind=\"line\")\n",
    "plt.xticks (rotation = 90)\n",
    "\n",
    "plt.savefig('model_train_time_performance.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3.11 Final processing of all model and prediction results. \n",
    "\n",
    "# Make recommendation based on market probability\n",
    "# 0.00 - 0.25: Highly not recommend; 0.25 -0.5: Not recommend; 0.5 - 0.75: Recommend; >=0.75 Highly recommend\n",
    "\n",
    "print  (list(total_asset_transactions.head(0)))\n",
    "\n",
    "def market_recommendation (row):\n",
    "    if row['Market_Probability'] >= 0.0000 and row['Market_Probability'] < 0.25:\n",
    "        return \"Highly Not Recommend\"\n",
    "    elif row['Market_Probability'] < 0.50:\n",
    "        return \"Not Recommend\"\n",
    "    elif row['Market_Probability'] < 0.75:\n",
    "        return \"Recommend\"\n",
    "    elif row['Market_Probability'] >= 0.75:\n",
    "        return \"Highly Recommend\"\n",
    "    else:\n",
    "        return np.NAN\n",
    "\n",
    "# Calculate and save the 'Market_Probability' based on the average probability value from the above 8 methods. \n",
    "total_asset_transactions['Market_Probability'] = 0.125 * (total_asset_transactions[\"Logistic_Regression_Accuracy_1\"] + total_asset_transactions[\"Decision_Tree_Accuracy_1\"] + total_asset_transactions[\"Neural_Network_Accuracy_1\"] + total_asset_transactions[\"SVM_Accuracy_1\"] + total_asset_transactions[\"Boosting_Accuracy_1\"]\n",
    "                                                 + total_asset_transactions[\"Linear_Regression_Probability\"] + total_asset_transactions[\"Bi_Normal_Distribution_Probability\"] + total_asset_transactions[\"Weibull_Distribution_Probability\"])\n",
    "# Calculate and save the 'Recommendation_Level' value based on the 'Market_Probability' value.\n",
    "total_asset_transactions['Recommendation_Level'] = total_asset_transactions.apply (lambda row: market_recommendation(row),axis=1)\n",
    "\n",
    "total_asset_transactions = total_asset_transactions [['AssetID', 'ClientCount', 'City', 'State', 'CountryID', 'CityID', 'State.1', 'Country', 'Zip', 'PropertyTypeID', 'PropertyType', 'PropertyTypeGroup', 'YearBuilt', 'AssetLatitude', 'AssetLongitude', 'LandAreaInAcres', 'NumberofFloors', 'NumberofBuildings', 'YearRenovated', 'Size_Acres', 'Size_InLineSqft', 'IsPortfolio', 'PortfolioCount', 'Lat_city', 'Long_city', 'IsGateway', 'Region', 'Size_Sqft', 'Size_Units', 'PropertySize', 'UnitType', 'ClassDateStamp', 'Market_name', 'CityID.1', 'DealID', 'AccountingDate', 'BuyerSelectionDate', 'InitialBidDueDate', 'MarketingDate', 'HireDate', 'Deal_Start_Date', 'Deal_End_Date', 'Current_Date', 'Prediction_Date', 'Year_Built', 'Year_Renovated', 'Date_Built', 'Date_Renovated', 'Date_from_Built', 'Date_from_Renovation', 'Date_to_Market', 'Date_to_Predict', 'Is_Transaction_Exist', 'Logistic_Regression', 'Logistic_Regression_Accuracy', 'Logistic_Regression_Accuracy_1', 'Decision_Tree', 'Decision_Tree_Accuracy', 'Decision_Tree_Accuracy_1', 'Neural_Network', 'Neural_Network_Accuracy', 'Neural_Network_Accuracy_1', 'Support_Vector_Machine', 'SVM_Accuracy', 'SVM_Accuracy_1', 'Boosting', 'Boosting_Accuracy', 'Boosting_Accuracy_1', 'Linear_Regression_Probability', 'Bi_Normal_Distribution_Probability', 'Weibull_Distribution_Probability', 'Market_Probability', 'Recommendation_Level']]\n",
    "total_asset_transactions.to_csv (\"Asset_Deal_Prediction_Full_Result.csv\")\n",
    "\n",
    "asset_market__probability_prediction = total_asset_transactions [['AssetID', 'Market_Probability', 'Recommendation_Level']]\n",
    "\n",
    "asset_market__probability_prediction = asset_market__probability_prediction[asset_market__probability_prediction.Market_Probability.notnull() & asset_market__probability_prediction.Recommendation_Level.notnull()].drop_duplicates()\n",
    "asset_market__probability_prediction.to_csv (\"Asset_Market_Prediction.csv\")\n",
    "\n",
    "print (\"Prediction about IA Assets 12-months to market finishes.\")\n",
    "\n",
    "\n",
    "### To do list: Other alterative analysis from the results of the 8 modeling methods. Examples: Voting, Weighted Combination. \n",
    "### Calculate the error of each model, find the best fit model, and use its result as the final prediciton result.\n",
    "### Use different combination (linear & non-linear) in parameter modeling and estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualize the final recommendation result.\n",
    "\n",
    "asset_market__probability_prediction = pd.read_csv(\"Asset_Market_Prediction.csv\")\n",
    "\n",
    "recommendation = ['Highly Recommend', 'Recommend', 'Not Recommend', 'Highly Not Recommend']\n",
    "count = []\n",
    "count.append(asset_market__probability_prediction[asset_market__probability_prediction.Recommendation_Level == 'Highly Recommend'].shape[0])\n",
    "count.append(asset_market__probability_prediction[asset_market__probability_prediction.Recommendation_Level == 'Recommend'].shape[0])\n",
    "count.append(asset_market__probability_prediction[asset_market__probability_prediction.Recommendation_Level == 'Not Recommend'].shape[0])\n",
    "count.append(asset_market__probability_prediction[asset_market__probability_prediction.Recommendation_Level == 'Highly Not Recommend'].shape[0])\n",
    "\n",
    "Recommendtion_counts = pd.DataFrame({\n",
    "    'Recommendation_Level' : recommendation,\n",
    "    'Count' : count })\n",
    "\n",
    "Recommendtion_counts.plot(x = \"Recommendation_Level\", y=\"Count\", kind=\"bar\")\n",
    "plt.savefig('final_recommendation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
